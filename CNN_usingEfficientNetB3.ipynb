{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1Hhvv_NS79g",
        "outputId": "07f8d655-c317-4112-e41e-f20768903c52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to load (likely expired) https://storage.googleapis.com/kaggle-data-sets/601280/1079953/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240419%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240419T073917Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=509d2c9ff33685edbc0fe9e443f8080432f7105182562df71a6acb61e0d1332aaf6b8324ff1ed43a2f2033627c181cdc5bf83dc6f49e1a93b735fc532684bb84288e8a087f33632e1517ae415822ee858d8d3c7ff1e075627f2102b3c3b2229d4a5ce1aad9d08c6de7cdd2240ac3a399aca56090b7ead8b230722ed43aee7f2598915741bd1aa044f31f9f1100299d4432b04e2a6e5356bad76acef9c94feaba5fe914d9645b50fcc8cca301264019298054b19dba59275b76848a0780376ed86266216d8f32cac24fb9d31d4cb99c4e112ddd8e9902bfa23846e65f7ab61105c418d807d2f6f3b6ecc5039a9b05a55f0651431410bf2127a0d7388f9e7b1c3b to path /kaggle/input/lung-and-colon-cancer-histopathological-images\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'lung-and-colon-cancer-histopathological-images:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F601280%2F1079953%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240419%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240419T073917Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D509d2c9ff33685edbc0fe9e443f8080432f7105182562df71a6acb61e0d1332aaf6b8324ff1ed43a2f2033627c181cdc5bf83dc6f49e1a93b735fc532684bb84288e8a087f33632e1517ae415822ee858d8d3c7ff1e075627f2102b3c3b2229d4a5ce1aad9d08c6de7cdd2240ac3a399aca56090b7ead8b230722ed43aee7f2598915741bd1aa044f31f9f1100299d4432b04e2a6e5356bad76acef9c94feaba5fe914d9645b50fcc8cca301264019298054b19dba59275b76848a0780376ed86266216d8f32cac24fb9d31d4cb99c4e112ddd8e9902bfa23846e65f7ab61105c418d807d2f6f3b6ecc5039a9b05a55f0651431410bf2127a0d7388f9e7b1c3b'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JylK6prXS79k"
      },
      "source": [
        "# **Import needed libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dw7hrznZS79m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam, Adamax\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.applications import EfficientNetB3\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3j98GvoS79n"
      },
      "outputs": [],
      "source": [
        "\n",
        "def loading_the_data(data_dir):\n",
        "\n",
        "    filepaths = []\n",
        "    labels = []\n",
        "\n",
        "\n",
        "    folds = os.listdir(data_dir)\n",
        "\n",
        "    for fold in folds:\n",
        "        foldpath = os.path.join(data_dir, fold)\n",
        "        filelist = os.listdir(foldpath)\n",
        "        for file in filelist:\n",
        "            fpath = os.path.join(foldpath, file)\n",
        "\n",
        "            filepaths.append(fpath)\n",
        "            labels.append(fold)\n",
        "\n",
        "    Fseries = pd.Series(filepaths, name='filepaths')\n",
        "    Lseries = pd.Series(labels, name='labels')\n",
        "\n",
        "    df = pd.concat([Fseries, Lseries], axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "def change_label_names(df, column_name):\n",
        "    index = {'lung_aca': 'Lung_adenocarcinoma', 'lung_n': 'Lung_benign_tissue', 'lung_scc': 'Lung squamous_cell_carcinoma'}\n",
        "\n",
        "\n",
        "    df[column_name] = df[column_name].replace(index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EY4zPRzjS79n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "f35af3ad-368d-4ee0-fb20-368960101a21"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/lung-and-colon-cancer-histopathological-images/lung_colon_image_set/lung_image_sets'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-6f45863d4fa3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/kaggle/input/lung-and-colon-cancer-histopathological-images/lung_colon_image_set/lung_image_sets'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloading_the_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mchange_label_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'labels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-b1c2c2e4d5c2>\u001b[0m in \u001b[0;36mloading_the_data\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/lung-and-colon-cancer-histopathological-images/lung_colon_image_set/lung_image_sets'"
          ]
        }
      ],
      "source": [
        "\n",
        "data_dir = '/kaggle/input/lung-and-colon-cancer-histopathological-images/lung_colon_image_set/lung_image_sets'\n",
        "df = loading_the_data(data_dir)\n",
        "\n",
        "change_label_names(df, 'labels')\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoYbVvfdS79o"
      },
      "outputs": [],
      "source": [
        "data_balance = df.labels.value_counts()\n",
        "\n",
        "\n",
        "def custom_autopct(pct):\n",
        "    total = sum(data_balance)\n",
        "    val = int(round(pct*total/100.0))\n",
        "    return \"{:.1f}%\\n({:d})\".format(pct, val)\n",
        "\n",
        "\n",
        "# pie chart for data balance\n",
        "plt.pie(data_balance, labels = data_balance.index, autopct=custom_autopct, colors = [\"#2092E6\",\"#6D8CE6\",\"#20D0E6\"])\n",
        "plt.title(\"Training data balance\")\n",
        "plt.axis(\"equal\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xqVwEBES79o"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_df, ts_df = train_test_split(df, train_size = 0.8, shuffle = True, random_state = 42)\n",
        "\n",
        "\n",
        "valid_df, test_df = train_test_split(ts_df, train_size = 0.5, shuffle = True, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkuLniEVS79p"
      },
      "outputs": [],
      "source": [
        "\n",
        "batch_size = 32\n",
        "img_size = (224, 224)\n",
        "\n",
        "tr_gen = ImageDataGenerator(rescale=1. / 255)\n",
        "ts_gen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "train_gen = tr_gen.flow_from_dataframe( train_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
        "                                    color_mode= 'rgb', shuffle= True, batch_size= batch_size)\n",
        "\n",
        "valid_gen = ts_gen.flow_from_dataframe( valid_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
        "                                    color_mode= 'rgb', shuffle= True, batch_size= batch_size)\n",
        "\n",
        "test_gen = ts_gen.flow_from_dataframe( test_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
        "                                    color_mode= 'rgb', shuffle= False, batch_size= batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K328NgknS79q"
      },
      "outputs": [],
      "source": [
        "g_dict = train_gen.class_indices\n",
        "classes = list(g_dict.keys())\n",
        "images, labels = next(train_gen)\n",
        "\n",
        "plt.figure(figsize= (20, 20))\n",
        "\n",
        "for i in range(batch_size):\n",
        "    plt.subplot(6, 6, i + 1)\n",
        "    image = images[i]\n",
        "    plt.imshow(image)\n",
        "    index = np.argmax(labels[i])  # get image index\n",
        "    class_name = classes[index]   # get class of image\n",
        "    plt.title(class_name, color= 'black', fontsize= 16)\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yZ3sNK0S79q"
      },
      "outputs": [],
      "source": [
        "\n",
        "def model_performance(history, Epochs):\n",
        "\n",
        "    tr_acc = history.history['accuracy']\n",
        "    tr_loss = history.history['loss']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    Epochs = [i+1 for i in range(len(tr_acc))]\n",
        "    plt.figure(figsize= (20, 8))\n",
        "    plt.style.use('fivethirtyeight')\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(Epochs, tr_loss, 'r', label= 'Training loss')\n",
        "    plt.plot(Epochs, val_loss, 'g', label= 'Validation loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')\n",
        "    plt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def model_evaluation(model):\n",
        "    train_score = model.evaluate(train_gen, verbose= 1)\n",
        "    valid_score = model.evaluate(valid_gen, verbose= 1)\n",
        "    test_score = model.evaluate(test_gen, verbose= 1)\n",
        "\n",
        "    print(\"Train Loss: \", train_score[0])\n",
        "    print(\"Train Accuracy: \", train_score[1])\n",
        "    print('-' * 20)\n",
        "    print(\"Validation Loss: \", valid_score[0])\n",
        "    print(\"Validation Accuracy: \", valid_score[1])\n",
        "    print('-' * 20)\n",
        "    print(\"Test Loss: \", test_score[0])\n",
        "    print(\"Test Accuracy: \", test_score[1])\n",
        "\n",
        "\n",
        "def get_pred(model, test_gen):\n",
        "\n",
        "    preds = model.predict(test_gen)\n",
        "    y_pred = np.argmax(preds, axis = 1)\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "# Confusion Matrix\n",
        "def plot_confusion_matrix(test_gen, y_pred):\n",
        "\n",
        "    g_dict = test_gen.class_indices\n",
        "    classes = list(g_dict.keys())\n",
        "\n",
        "    cm = confusion_matrix(test_gen.classes, y_pred)\n",
        "\n",
        "    plt.figure(figsize= (10, 10))\n",
        "    plt.imshow(cm, interpolation= 'nearest', cmap= plt.cm.Blues)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.colorbar()\n",
        "\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation= 45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j], horizontalalignment= 'center', color= 'white' if cm[i, j] > thresh else 'black')\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def conv_block(filters, act='relu'):\n",
        "\n",
        "    block = Sequential()\n",
        "    block.add(Conv2D(filters, 3, activation=act, padding='same'))\n",
        "    block.add(Conv2D(filters, 3, activation=act, padding='same'))\n",
        "    block.add(BatchNormalization())\n",
        "    block.add(MaxPooling2D())\n",
        "\n",
        "    return block\n",
        "\n",
        "def dense_block(units, dropout_rate, act='relu'):\n",
        "\n",
        "    block = Sequential()\n",
        "    block.add(Dense(units, activation=act))\n",
        "    block.add(BatchNormalization())\n",
        "    block.add(Dropout(dropout_rate))\n",
        "\n",
        "    return block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJTsetUrS79r"
      },
      "outputs": [],
      "source": [
        "\n",
        "img_size = (224, 224)\n",
        "channels = 3\n",
        "img_shape = (img_size[0], img_size[1], channels)\n",
        "\n",
        "class_counts = len(list(train_gen.class_indices.keys()))     # to define number of classes in dense layer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN"
      ],
      "metadata": {
        "id": "6YeJeGtmZ5kh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENS2oWZQS79r"
      },
      "outputs": [],
      "source": [
        "\n",
        "cnn_model = Sequential()\n",
        "\n",
        "\n",
        "cnn_model.add(Conv2D(filters=16, kernel_size=(3,3), padding=\"same\", activation=\"relu\", input_shape= img_shape))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(MaxPooling2D())\n",
        "\n",
        "\n",
        "cnn_model.add(conv_block(32))\n",
        "\n",
        "cnn_model.add(conv_block(64))\n",
        "\n",
        "cnn_model.add(conv_block(128))\n",
        "\n",
        "cnn_model.add(conv_block(256))\n",
        "\n",
        "cnn_model.add(Flatten())\n",
        "\n",
        "cnn_model.add(dense_block(128, 0.5))\n",
        "\n",
        "cnn_model.add(dense_block(64, 0.3))\n",
        "\n",
        "cnn_model.add(dense_block(32, 0.2))\n",
        "\n",
        "cnn_model.add(Dense(class_counts, activation = \"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iHa7PVsS79s"
      },
      "outputs": [],
      "source": [
        "cnn_model.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n",
        "\n",
        "cnn_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rQiZkD3S79s"
      },
      "outputs": [],
      "source": [
        "\n",
        "epochs = 20\n",
        "\n",
        "history = cnn_model.fit(train_gen, epochs= epochs, verbose= 1, validation_data= valid_gen, shuffle= False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxr9HOVGS79s"
      },
      "source": [
        "**Display model performance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlTQA5BoS79s"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_performance(history, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2ZX-GocS79s"
      },
      "source": [
        "**Evaluate the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NE7zYCovS79s"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_evaluation(cnn_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE7zJ5dgS79s"
      },
      "source": [
        "**Get predictions and display the confusion matrix**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Io9D8-9PYUlR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKQtDCg1S79t"
      },
      "outputs": [],
      "source": [
        "\n",
        "y_pred = get_pred(cnn_model, test_gen)\n",
        "\n",
        "plot_confusion_matrix(test_gen, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    # Load and preprocess the dataset (example using MNIST)\n",
        "    (X_train, y_train), (X_valid, y_valid) = mnist.load_data()\n",
        "    X_train = X_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "    X_valid = X_valid.reshape(-1, 28, 28, 1) / 255.0\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, class_counts)\n",
        "    y_valid = tf.keras.utils.to_categorical(y_valid, class_counts)\n",
        "\n",
        "    # Create model\n",
        "    model = create_model(trial)\n",
        "\n",
        "    # Train the model\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    history = model.fit(X_train, y_train,\n",
        "                        epochs=20,\n",
        "                        validation_data=(X_valid, y_valid),\n",
        "                        callbacks=[early_stopping],\n",
        "                        verbose=1)\n",
        "\n",
        "    # Evaluate the model\n",
        "    score = model.evaluate(X_valid, y_valid, verbose=0)\n",
        "    return score[0]  # Return validation loss\n",
        "\n",
        "# Run the hyperparameter optimization\n",
        "def optimize_hyperparameters():\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(objective, n_trials=10)  # Adjust n_trials as needed\n",
        "    print(\"Number of finished trials:\", len(study.trials))\n",
        "    print(\"Best trial:\", study.best_trial.params)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    optimize_hyperparameters()"
      ],
      "metadata": {
        "id": "hgiUTw60Ypkn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 601280,
          "sourceId": 1079953,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30665,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}